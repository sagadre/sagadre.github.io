<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>sy</title>

  <meta name="author" content="Samir Yitzhak Gadre">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.ico">
</head>

<style>
  body {
    background-repeat: repeat;
  }
</style>

<body onload="toggleNews();toggleNews()" background="images/noise_opt.png">
  <!-- <header id="header" style="display:flex; justify-content: space-between; padding-left:10px;padding-right:10px;"></header> -->

  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Samir Yitzhak Gadre</name>
                  </p>
                  <p>
                    I am a 4rd year Ph.D. student at Columbia University studying large-scale dataset construction and
                    model training. I am privileged to be advised by <a
                      href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a> and
                    fortunate to work closely with <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>.
                  </p>
                  <p>
                    My work is supported by a Columbia Presidential Fellowship and a NSF Graduate Research Fellowship.
                  </p>
                  <p>
                    In addition to research, I enjoy running, climbing mountains, and singing with my pop/rock choir:
                    <a href="https://www.heretosing.com/">Here to Sing</a>.
                  </p>
                  <!-- Before Columbia, I was a software engineer at <a
                    href="https://www.microsoft.com/en-us/hololens">Microsoft HoloLens</a>. I was on the Object
                  Understanding team, where I was lucky to work with <a
                    href="https://www.linkedin.com/in/harpreet-sawhney-2b024b28/">Harpreet Sawhney</a> and <a
                    href="https://www.linkedin.com/in/ning-xu-5a687896/">Ning Xu</a> on <a
                    href="https://azure.microsoft.com/en-us/services/object-anchors/#overview">Object Anchors</a>.
                  <p>
                    Prior to Microsoft, I was an undergrad at Brown University. I had a fantastic
                    experience working with <a href="http://cs.brown.edu/people/gdk/">George Konidaris</a> and <a
                      href="http://cs.brown.edu/people/stellex/">Stefanie Tellex</a> on augmented reality robot
                    programming and learning
                    from demonstration.
                  </p> -->
                  <p style="text-align:center">
                    <a href="mailto:sy@cs.columbia.edu">Email</a> &nbsp/&nbsp
                    <!-- <a href="data/gadre_cv.pdf">CV</a> &nbsp|&nbsp -->
                    <a href="https://scholar.google.com/citations?user=oAhlg9gAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/sy_gadre">Twitter</a>
                    <!-- <a href="data/gadre_cv.pdf">CV</a> -->
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg"><img style="width:75%;max-width:75%" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <ul>
                <li>[Aug 2023] New preprint: <a href="https://arxiv.org/abs/2308.01390">OpenFlamingo</a>, led by <a
                    href="https://anas-awadalla.streamlit.app/">Anas Awadalla</a> and <a
                    href="http://i-gao.github.io/">Irena Gao</a>.</li>
                <li>[July 2023] New preprint: <a href="https://arxiv.org/abs/2307.10350">Improving multimodal datasets
                    with image captioning</a>, led by <a href="https://thaonguyen19.github.io/">Thao Nguyen</a> (NeurIPS
                  2023).</li>
                <li>[July 2023] New preprint: <a href="https://arxiv.org/abs/2307.05663">Objaverse-XL</a>, led by <a
                    href="https://mattdeitke.com/">Matt Deitke</a> (NeurIPS 2023).</li>
                <li>[Apr 2023] New preprint: <a href="https://arxiv.org/abs/2304.14108">DataComp</a> (NeurIPS 2023).
                </li>
                <li>[Apr 2023] New preprint: <a href="https://arxiv.org/abs/2304.06939">Multimodal C4 (MMC4)</a>, led by
                  <a href="https://wanrong-zhu.com/">Wanrong Zhu</a> and <a href="https://jmhessel.com/">Jack
                    Hessel</a> (NeurIPS 2023).
                </li>
                <span id="moreNews">
                  <li>[Aug 2022] New preprint: <a href="https://arxiv.org/abs/2208.05592">Patching open-vocabulary
                      models
                      by interpolating weights (PAINT)</a> (NeurIPS 2022).</li>
                  <li>[July 2022] New preprint: <a href="https://arxiv.org/abs/2207.08997">Structure from Action
                      (SfA)</a>, led by <a href="https://neilnie.com/">Neil Nie</a> (IROS 2023).</li>
                  <li>[June 2022] I'll be at CVPR! Check out the <a
                      href="https://sites.google.com/view/cvpr2022-robot-learning">Vision-based Robot Learning
                      Tutorial</a> I helped organize.</li>
                  <li>[May 2022] Returning to <a href="https://prior.allenai.org/">AI2
                      PRIOR</a> for another internship.</li>
                  <li>[Mar 2022] New preprint: <a href="https://arxiv.org/abs/2203.10421">CLIP on Wheels (CoW)</a> (CVPR
                    2023).</li>
                  <li>[Mar 2022] New preprint: <a href="https://arxiv.org/abs/2203.05482">Model soups</a>, led by <a
                      href="https://mitchellnw.github.io/">Mitchell Wortsman</a> (ICML 2022).</li>
                  <li>[Mar 2022] New preprint: <a href="https://arxiv.org/abs/2203.17251">Continuous Scene
                      Representations</a>, a
                    collaboration with AI2 (CVPR 2022).</li>
                  <li>[June 2021] Started an internship with the amazing folks at <a
                      href="https://prior.allenai.org/">AI2
                      PRIOR</a>.</li>
                  <li>[May 2021] New preprint: <a href="https://arxiv.org/abs/2105.01047">Act the Part (AtP)</a> (ICCV
                    2021).
                  </li>
                  <li>[Sept 2020] Joined Shuran Song's group at Columbia. Excited to be a Ph.D. student!</li>

                  <li>[July 2020] Promoted to Software Engineer II at Microsoft HoloLens.</li>
                  <li>[May 2019] Presented our robot programing mixed reality work at ICRA 2019.</li>
                  <li>[Feb 2019] Joined the Object Understanding team at Microsoft HoloLens.</li>
                  <li>[Oct 2018] Spoke at the <a
                      href="https://robotics.cs.washington.edu/colloquium/archive/18au/">University of Washington
                      robotics colloquium</a> about mixed reality human-robot interaction.</li>
                  <li>[May 2018] Graduated from Brown with an Sc.B. in computer science (with honors) and an A.B. in
                    engineering.</li>
                </span>
                <div onclick="toggleNews()" id="moreNewsBtn" class="showBtn"><a>Show more...</a></div>
                <div onclick="toggleNews()" id="lessNewsBtn" class="showBtn"><a>Show less...</a></div>
                <div style="clear: both;"></div>
              </ul>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Talks</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="5">
            <tbody>
              <tr>
                <td width="75%" valign="center">
                  CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation [<a
                    href="data/cow_cvpr_2023.pdf" loading="lazy">slides</a>] at the <a
                    href="https://scene-understanding.com/">CVPR 2023 Workshop on 3D Scene
                    Understanding for Vision, Graphics, and Robotics</a>.
                </td>
              </tr>
              <tr>
                <td width="75%" valign="center">
                  No Training? Towards Adapting Zero-Shot Models to Robotics Tasks [<a
                    href="data/no_training_tutorial_talk.pdf" loading="lazy">slides</a>] at the <a
                    href="https://sites.google.com/view/cvpr2022-robot-learning">CVPR 2022 Tutorial on Vision-Based
                    Robot Learning</a>.
                </td>
              </tr>
            </tbody>
          </table>

          <!-- https://scene-understanding.com/ -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications and Pre-Prints</heading>
                  <br> (* indicates equal contribution)
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/flamingo-llama.png" , height="120" , width="180">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2308.01390">
                    <papertitle>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language
                      Models
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://anas-awadalla.streamlit.app/">Anas Awadalla*</a>,
                  <a href="http://i-gao.github.io/">Irena Gao*</a>,
                  <a href="https://homes.cs.washington.edu/~jpgard/">Josh Gardner</a>,
                  <a href="https://jmhessel.com/">Jack Hessel</a>,
                  Yusuf Hanafy,
                  <a href="https://wanrong-zhu.com/">Wanrong Zhu</a>,
                  <a href="https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0">Kalyani Marathe</a>,
                  <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a>,
                  <strong>Samir Yitzhak Gadre</strong>,
                  <a href="https://cs.stanford.edu/~ssagawa/">Shiori Sagawa</a>,
                  <a href="https://de.linkedin.com/in/jenia-jitsev-11654427">Jenia Jitsev</a>,
                  <a href="https://simonster.com/">Simon Kornblith</a>,
                  <a href="https://koh.pw/">Pang Wei Koh</a>,
                  <a href="http://gabrielilharco.com/">Gabriel Ilharco</a>,
                  <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>,
                  <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2308.01390">arXiv</a> |
                  <a href="https://laion.ai/blog/open-flamingo-v2/">blog</a> |
                  <a href="https://github.com/mlfoundations/open_flamingo">code</a> |
                  <a href="https://huggingface.co/spaces/openflamingo/OpenFlamingo">demo</a>
                  <p>
                    An open-source implementation of Flamingo models and training.
                  </p>
                </td>
              </tr>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/blip2.jpg" , height="120" , width="180">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2307.10350">
                    <papertitle>Improving multimodal datasets with image captioning
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://thaonguyen19.github.io/">Thao Nguyen</a>,
                  <strong>Samir Yitzhak Gadre</strong>,
                  <a href="http://gabrielilharco.com/">Gabriel Ilharco</a>,
                  <a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a>,
                  <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2307.10350">arXiv</a> | More coming soon!
                  <!-- <a href="https://datacomp.ai">website</a> |
                  <a href="https://github.com/mlfoundations/datacomp">code</a> -->
                  <p>
                    Improving image-text datasets for downstream classification and retrieval using image captioning
                    models (e.g., BLIP2).
                  </p>
                </td>
              </tr>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/obj.png" , height="120" , width="180">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2307.05663">
                    <papertitle>Objaverse-XL: A Universe of 10M+ 3D Objects
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://mattdeitke.com/">Matt Deitke</a>,
                  <a href="https://ruoshiliu.github.io/">Ruoshi Liu</a>,
                  <a href="https://mattwallingford.github.io/">Matthew Wallingford</a>,
                  <a href="https://www.linkedin.com/in/huong-ngo-016837210/">Huong Ngo</a>,
                  <a href="https://ojmichel.github.io/">Oscar Michel</a>,
                  <a href="https://homes.cs.washington.edu/~kusupati/">Aditya Kusupati</a>,
                  <a href="">Alan Fan</a>,
                  <a href="https://ca.linkedin.com/in/claforte">Christian Laforte</a>,
                  <a href="https://voletiv.github.io/">Vikram Voleti</a>,
                  <strong>Samir Yitzhak Gadre</strong>,
                  <a href="https://www.linkedin.com/in/eli-vanderbilt-a9710716">Eli VanderBilt</a>,
                  <a href="https://anikem.github.io/">Aniruddha Kembhavi</a>,
                  <a href="http://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>,
                  <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>,
                  <a href="https://ehsanik.github.io/">Kiana Ehsani</a>,
                  <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt*</a>,
                  <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi*</a>
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2307.05663">arXiv</a> |
                  <a href="https://objaverse.allenai.org/">website</a> |
                  <a href="https://github.com/allenai/objaverse-xl">code</a>
                  <p>
                    A dataset of over 10 million 3D objects.
                  </p>
                </td>
              </tr>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/datacomp.png" , height="50" , width="180">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2304.14108">
                    <papertitle>DataComp: In search of the next generation of multimodal datasets
                    </papertitle>
                  </a>
                  <br>
                  <strong>Samir Yitzhak Gadre*</strong>,
                  <a href="http://gabrielilharco.com/">Gabriel Ilharco*</a>,
                  Alex Fang*,
                  <a href="https://www.linkedin.com/in/jonathan-hayase-5ab849128">Jonathan Hayase</a>,
                  <a href="https://georgiossmyrnis.github.io/">Georgios Smyrnis</a>,
                  <a href="https://thaonguyen19.github.io/">Thao Nguyen</a>,
                  <a href="https://www.ryanmarten.com/">Ryan Marten</a>,
                  <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>,
                  <a href="https://djghosh13.github.io/">Dhruba Ghosh</a>,
                  <a href="https://jieyuz2.github.io/">Jieyu Zhang</a>,
                  <a href="">Eyal Orgad</a>,
                  <a href="https://rahimentezari.github.io/">Rahim Entezari</a>,
                  <a href="https://giannisdaras.github.io/">Giannis Daras</a>,
                  <a href="https://sarahpratt.github.io/">Sarah Pratt</a>,
                  <a href="https://vkramanuj.github.io/">Vivek Ramanujan</a>,
                  <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a>,
                  <a href="https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0">Kalyani Marathe</a>,
                  <a href="https://stephen.mussmann.xyz/">Stephen Mussmann</a>,
                  <a href="https://ro.linkedin.com/in/richardvencu">Richard Vencu</a>,
                  <a href="https://mehdidc.github.io/">Mehdi Cherti</a>,
                  <a href="http://www.ranjaykrishna.com/index.html">Ranjay Krishna</a>,
                  <a href="https://koh.pw/">Pang Wei Koh</a>,
                  <a href="http://www.olgasaukh.com/">Olga Saukh</a>,
                  <a href="https://ajratner.github.io/">Alexander Ratner</a>,
                  <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>,
                  <a href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a>,
                  <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a>,
                  <a href="https://rom1504.fr/">Romain Beaumont</a>,
                  <a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a>,
                  <a href="https://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a>,
                  <a href="https://de.linkedin.com/in/jenia-jitsev-11654427">Jenia Jitsev</a>,
                  <a href="https://www.cs.tau.ac.il/~ycarmon/">Yair Carmon</a>,
                  <a href="http://vaishaal.com/">Vaishaal Shankar</a>,
                  <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>
                  <br>
                  <em>NeurIPS</em>, 2023 <b>(oral)</b>
                  <br>
                  <a href="https://arxiv.org/abs/2304.14108">arXiv</a> |
                  <a href="https://datacomp.ai">website</a> |
                  <a href="https://github.com/mlfoundations/datacomp">code</a>
                  <p>
                    A benchmark where model training is fixed and participants iterate on data curation strategies. We
                    release a dataset of 12.8B image-text pairs, the largest public dataset of its kind to date. On a
                    1.4B
                    subset, DataComp-1B, we outperform OpenAI CLIP models trained with the same compute budget.
                  </p>
                </td>
              </tr>

              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmc4_logo.png" , height="50" , width="180">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2304.06939">
                  <papertitle>Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text
                  </papertitle>
                </a>
                <br>
                <a href="https://wanrong-zhu.com/">Wanrong Zhu*</a>,
                <a href="https://jmhessel.com/">Jack Hessel*</a>,
                <a href="https://anas-awadalla.streamlit.app/">Anas Awadalla</a>,
                <strong>Samir Yitzhak Gadre</strong>,
                <a href="https://jessedodge.github.io/">Jesse Dodge</a>,
                Alex Fang,
                <a href="https://yj-yu.github.io/home/">Youngjae Yu</a>,
                <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>,
                <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a>,
                <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2304.06939">arXiv</a> | <a
                  href="https://github.com/allenai/mmc4">code</a>
                <p>
                  A billion-scale dataset of interleaved images and text.
                </p>
              </td>
      </tr>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/cow2.jpg" , height="120" , width="180">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2203.10421">
            <papertitle>CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object
              Navigation
            </papertitle>
          </a>
          <br>
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>,
          <a href="http://gabrielilharco.com/">Gabriel Ilharco</a>,
          <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>,
          <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2203.10421">arXiv</a> | <a href="https://cow.cs.columbia.edu/">website</a> | <a
            href="https://github.com/columbia-ai-robotics/cow">code</a>
          <p>
            We study how to turn existing zero-shot vision-and-language models (e.g., CLIP) into zero-shot
            object navigators.
          </p>
        </td>
      </tr>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:35px;width:25%;vertical-align:middle">
          <img src="images/paint.jpeg" , height="120" , width="150">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2208.05592">
            <papertitle>Patching open-vocabulary models by interpolating weights
            </papertitle>
          </a>
          <br>
          <a href="http://gabrielilharco.com/">Gabriel Ilharco*</a>,
          <a href="https://mitchellnw.github.io/">Mitchell Wortsman*</a>,
          <strong>Samir Yitzhak Gadre*</strong>,
          <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>,
          <a href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a>,
          <a href="https://simonster.com/">Simon Kornblith</a>,
          <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a>,
          <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2208.05592">arXiv</a> | <a href="https://model-patching.github.io/">website
            (with demo!)</a> | <a href="https://github.com/mlfoundations/patching">code</a>
          <p>
            We introduce PAINT to improve performance on tasks where pre-trained open-vocabulary models
            struggle, while maintaining performance on tasks they are already performant on.
          </p>
        </td>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/drawer.gif" , height="120" , width="180">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2207.08997">
            <papertitle>Structure From Action: Learning Interactions for Articulated Object 3D Structure
              Discovery
            </papertitle>
          </a>
          <br>
          <a href="https://neilnie.com/">Neil Nie</a>,
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="https://ehsanik.github.io/">Kiana Ehsani</a>,
          <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
          <br>
          <em>IROS</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2207.08997">arXiv</a> | More coming soon!
          <p>
            We learn how to interact with 3D articulated objects to reconstruct their parts and discover joint
            constraints.
          </p>
        </td>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/soup.jpeg" width="180" height="120">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2203.05482">
            <papertitle>Model soups: averaging weights of multiple fine-tuned models improves accuracy without
              increasing inference time</papertitle>
          </a>
          <br>
          <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>,
          <a href="http://gabrielilharco.com/">Gabriel Ilharco</a>,
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="https://twitter.com/beccaroelofs">Rebecca Roelofs</a>,
          <a href="https://raphagl.com/">Raphael Gontijo-Lopes</a>,
          <a href="http://www.arimorcos.com/">Ari S. Morcos</a>,
          <a href="https://hsnamkoong.github.io/">Hongseok Namkoong</a>,
          <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a>,
          <a href="https://www.cs.tau.ac.il/~ycarmon/">Yair Carmon*</a>,
          <a href="https://simonster.com/">Simon Kornblith*</a>,
          <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt*</a>
          <br>
          <em>ICML</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.05482">arXiv</a> | <a
            href="https://github.com/mlfoundations/model-soups">code</a>
          <p>
            We average the weights of many models (ingredients) fine-tuned with different hyperparameters. The
            resulting soup outperforms the individual ingredients.
          </p>
        </td>
      </tr>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/csr.jpg" width="180" height="120">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2203.17251">
            <papertitle>Continuous Scene Representations for Embodied AI</papertitle>
          </a>
          <br>
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="https://ehsanik.github.io/">Kiana Ehsani</a>,
          <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>,
          <a href="https://roozbehm.info/">Roozbeh Mottaghi</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.17251">arXiv</a> | <a
            href="https://prior.allenai.org/projects/csr">website</a> | <a
            href="https://github.com/allenai/CSR">code</a>
          <p>
            We employ a contrastive loss to embed relationships between objects as features. We show how our
            representation can be used downstream for visual room rearrangement without any additional training.
            <!-- We learn multi-step interaction with articulated objects to (1) discover the number of parts and (2) find part segmentation masks, all without semantic labels. -->
          </p>
        </td>
      </tr>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/atp.jpeg">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2105.01047">
            <papertitle>Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery
            </papertitle>
          </a>
          <br>
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="https://ehsanik.github.io/">Kiana Ehsani</a>,
          <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2105.01047">arXiv</a> |
          <a href="https://atp.cs.columbia.edu/">website (with demo!)</a>
          <p>
            We learn multi-step interaction with articulated objects to (1) discover the number of parts and (2)
            find part segmentation masks, all without semantic labels.
          </p>
        </td>
      </tr>

      <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/end_user.gif">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://cs.brown.edu/people/gdk/pubs/end_user_prog_mr.pdf">
            <papertitle>End-User Robot Programming Using Mixed Reality</papertitle>
          </a>
          <br>
          <strong>Samir Yitzhak Gadre</strong>,
          <a href="http://cs.brown.edu/people/er35/">Eric Rosen</a>,
          <a href="https://www.linkedin.com/in/garychien/">Gary Chien</a>,
          <a href="https://www.elizabethkphillips.com/">Elizabeth Phillips</a>,
          <a href="http://cs.brown.edu/people/stellex/">Stefanie Tellex</a>,
          <a href="http://cs.brown.edu/people/gdk/">George Konidaris</a>
          <br>
          <em>ICRA</em>, 2019
          <br>
          <a href="https://cs.brown.edu/people/gdk/pubs/end_user_prog_mr.pdf">pdf</a>
          <p>
            Mixed reality robot programming interface for pick-and-place tasks.
          </p>
        </td>
      </tr>


      <tr onmouseout="lfd_stop()" onmouseover="lfd_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/lfd.gif">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://cs.brown.edu/research/pubs/theses/ugrad/2018/gadre.samir.pdf">
            <papertitle>Teaching Robots Using Mixed Reality</papertitle>
          </a>
          <br>
          <strong>Samir Yitzhak Gadre</strong>
          <br>
          <em>Brown University Undergraduate Honors Thesis</em>, 2018
          <br>
          <a href="http://cs.brown.edu/research/pubs/theses/ugrad/2018/gadre.samir.pdf">pdf</a>
          <p>
            Mixed reality learning from demonstration system for pick-and-place tasks.
          </p>
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/review.jpg" height="166" ,
                width="166"></td>
            <td width="75%" valign="center">

              ECCV: 2020; CVPR: 2023; ICLR: 2023; ICML: 2022, 2023; IROS: 2022; ICRA: 2023; NeurIPS: 2023
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center"><img src="images/columbia.jpg"
                height="166" , width="166"></td>
            <td width="75%" valign="center">
              Pre-submission Application Review (PAR); co-organizer, fall 2021.
              <br>
              <br>
              <a href="https://www.womeninscienceatcolumbia.org/">WiSC</a>; mentor, fall 2020, spring 2021.
              <br>
              <br>
              PAR; application reader, fall 2020.
              <br>
              <br>
              <a href="https://sites.google.com/view/coms4733-fall2020/home?pli=1&authuser=1">COMS 4733:
                Computational Aspects of Robotics</a>; Graduate Teaching Assistant, fall 2020.
              <br>
              <br>
              COMS 6998: Topics in Robot Learning; Graduate Teaching Assistant, spring 2021.
              <a></a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center"><img src="images/brown_cs.jpg">
            </td>
            <td width="75%" valign="center">
              <a href="http://cs.brown.edu/courses/cs016/">CS16: Algorithms and Data Structures</a>; Teaching
              Assistant, spring 2018.
              <br>
              <br>
              <a href="http://cs.brown.edu/courses/cs015/">CS15: Object Oriented Programming</a>; Teaching
              Assistant, fall 2016.

            </td>
          </tr>
        </tbody>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>More!</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/review.jpg" height="166" ,
                        width="166"></td> -->
            <td width="75%" valign="center">
              Releasing my NSF GRFP application, inspired by others, whose materials were super helpful to me
              [<a href="data/nsf_personal.pdf">personal</a>, <a href="data/nsf_research.pdf">research</a>]
            </td>
          </tr>
        </tbody>
      </table>

      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <p style="text-align:right;font-size:small;">
                Template modified from the <a href="https://github.com/jonbarron/jonbarron_website">Jon
                  Barron</a> original.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
  </table>

  <script>
    function toggleNews() {
      var moreNews = document.getElementById("moreNews");
      var moreNewsBtn = document.getElementById("moreNewsBtn");
      var lessNewsBtn = document.getElementById("lessNewsBtn");
      if (moreNewsBtn.style.display === "none") {
        moreNews.style.display = "none";
        moreNewsBtn.style.display = "inline";
        lessNewsBtn.style.display = "none";
      } else {
        moreNews.style.display = "inline";
        moreNewsBtn.style.display = "none";
        lessNewsBtn.style.display = "inline";
      }
    }
  </script>
</body>

</html>